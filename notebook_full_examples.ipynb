{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### `initialize`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### prequesite files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# mkdir /tmp/snippets if not exists\n",
        "if not os.path.exists('/tmp/snippets'):\n",
        "    os.makedirs('/tmp/snippets')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('''\n",
        "cp ~/Downloads/snippets/actions_with_embeddings.csv /tmp/snippets/\n",
        "cp ~/Downloads/snippets/actions.csv /tmp/snippets/\n",
        "cp ~/Downloads/snippets/connection_schema.csv /tmp/snippets/\n",
        "cp ~/Downloads/snippets/fuzz_run_search.py /tmp/snippets/\n",
        "''')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from openai import OpenAI\n",
        "import pandas as pd\n",
        "import duckdb\n",
        "\n",
        "conn = duckdb.connect(database=':memory:')\n",
        "conn.sql(f\"create table metadata as select * from read_csv_auto('/tmp/snippets/connection_schema.csv')\")\n",
        "\n",
        "client = OpenAI(max_retries=5)\n",
        "EMBEDDING_MODEL=\"text-embedding-ada-002\"\n",
        "\n",
        "def cosine_similarity(a, b):\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "def get_embedding(text, model=EMBEDDING_MODEL):\n",
        "   text = text.replace(\"\\n\", \" \")\n",
        "   return client.embeddings.create(input = [text], model=model).data[0].embedding\n",
        "\n",
        "def search_docs(df, user_query, top_n=3, to_print=True):\n",
        "    embedding = get_embedding(\n",
        "        user_query,\n",
        "        model=EMBEDDING_MODEL \n",
        ")\n",
        "\n",
        "    df[\"similarities\"] = df.saved_embedding.apply(lambda x: cosine_similarity(x, embedding))\n",
        "    print(df.head(10))\n",
        "    res = (\n",
        "        df.sort_values(\"similarities\", ascending=False)\n",
        "        .head(top_n)\n",
        "    )\n",
        "    if to_print:\n",
        "        display(res)\n",
        "    return res\n",
        "\n",
        "def get_next_task(df, user_query, top_n=3, to_print=True):\n",
        "    s = search_docs(df, user_query, top_n=top_n, to_print=to_print)\n",
        "    return s\n",
        "\n",
        "import ast\n",
        "docs_df = pd.read_csv(\"/tmp/snippets/actions_with_embeddings.csv\")\n",
        "docs_df.dropna()\n",
        "docs_df['saved_embedding'] = docs_df['embedding'].apply(ast.literal_eval)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import os\n",
        "import autogen\n",
        "\n",
        "# Gemini imports \n",
        "import chromadb\n",
        "from vertexai.generative_models import HarmBlockThreshold, HarmCategory\n",
        "from autogen import ConversableAgent, AssistantAgent, UserProxyAgent\n",
        "from typing_extensions import Annotated\n",
        "from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union\n",
        "from autogen import Agent, AssistantAgent, ConversableAgent, UserProxyAgent\n",
        "from autogen.agentchat.contrib.img_utils import _to_pil, get_image_data\n",
        "#from autogen.agentchat.contrib.multimodal_conversable_agent import MultimodalConversableAgent\n",
        "from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent\n",
        "from autogen.code_utils import DEFAULT_MODEL, UNKNOWN, content_str, execute_code, extract_code, infer_lang\n",
        "safety_settings = {\n",
        "    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
        "    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
        "    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
        "    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
        "}\n",
        "\n",
        "# Env\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "project_id=\"sample-1474250537486\"\n",
        "\n",
        "import google.auth\n",
        "\n",
        "scopes = [\"https://www.googleapis.com/auth/cloud-platform\"]\n",
        "creds, project = google.auth.default(scopes)\n",
        "auth_req = google.auth.transport.requests.Request()\n",
        "creds.refresh(auth_req)\n",
        "prompt_price_per_1k = (\n",
        "    0.000125  \n",
        ")\n",
        "\n",
        "completion_token_price_per_1k = (\n",
        "    0.000375  # For more up-to-date prices see https://cloud.google.com/vertex-ai/generative-ai/pricing\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Models\n",
        "cerebras_config_list = [\n",
        "{\n",
        "        \"model\": \"llama3.3-70b\",\n",
        "        \"api_key\": os.environ.get(\"CEREBRAS_API_KEY\"),\n",
        "        \"api_type\": \"cerebras\",\n",
        "        \"stream\" : False,\n",
        "        \"base_url\": \"https://api.cerebras.ai/v1\",\n",
        "        \"temperature\": 0.0\n",
        "}\n",
        "]\n",
        "\n",
        "gemini_config_list = [\n",
        "    {\n",
        "        \"model\": \"gemini-2.0-flash-exp\",\n",
        "        \"api_type\": \"google\",\n",
        "        \"project_id\": project_id,\n",
        "        \"location\": \"us-central1\",\n",
        "        \"google_application_credentials\": \"/Users/brian/key.json\",\n",
        "        \"api_rate_limit\" : 1,\n",
        "        \"safety_settings\": safety_settings,\n",
        "        \"temperature\": 0.0,\n",
        "        \"max_tokens\": 7000\n",
        "    },\n",
        "]\n",
        "\n",
        "oai_config_list = [\n",
        "    {\n",
        "        \"model\": \"gpt-4o\", \n",
        "        \"api_key\": os.environ[\"OPENAI_API_KEY\"],\n",
        "        \"temperature\": 0.0, \n",
        "        \"stream\": False\n",
        "    }\n",
        "]\n",
        "\n",
        "groq_config_list = [\n",
        "    {\n",
        "        \"model\": \"llama3-70b-8192\",\n",
        "        \"api_key\": os.environ[\"GROQ_API_KEY\"],\n",
        "        \"api_type\": \"groq\",\n",
        "        \"frequency_penalty\": 0.5,\n",
        "        \"max_tokens\": 2048,\n",
        "        \"presence_penalty\": 0.2,\n",
        "        \"seed\": 42,\n",
        "        \"temperature\": 0.0,\n",
        "        \"top_p\": 0.2\n",
        "    }\n",
        "]\n",
        "\n",
        "anthropic_config_list = [\n",
        "    {\n",
        "        \"model\": \"claude-3-5-sonnet-20240620\",\n",
        "        \"api_key\": os.environ[\"ANTHROPIC_API_KEY\"],\n",
        "        \"api_type\": \"anthropic\",\n",
        "    },\n",
        "]\n",
        "\n",
        "cerebras_small_config_list = [\n",
        "{\n",
        "        \"model\": \"llama3.3-8b\",\n",
        "        \"api_key\": os.environ.get(\"CEREBRAS_API_KEY\"),\n",
        "        \"api_type\": \"cerebras\",\n",
        "        \"stream\" : False,\n",
        "        \"base_url\": \"https://api.cerebras.ai/v1\",\n",
        "        \"temperature\": 0.0\n",
        "}\n",
        "]\n",
        "\n",
        "groq_small_config_list = [\n",
        "    {\n",
        "        \"model\": \"llama3-8b-8192\",\n",
        "        \"api_key\": os.environ[\"GROQ_API_KEY\"],\n",
        "        \"api_type\": \"groq\",\n",
        "        \"frequency_penalty\": 0.5,\n",
        "        \"max_tokens\": 2048,\n",
        "        \"presence_penalty\": 0.2,\n",
        "        \"seed\": 42,\n",
        "        \"temperature\": 0.0,\n",
        "        \"top_p\": 0.2\n",
        "    }\n",
        "]\n",
        "\n",
        "ollama_config_list = [\n",
        "    {\n",
        "        'model': 'llama3.2:1b', #'smollm2:1.7b',\n",
        "        'api_key' : 'ollama',\n",
        "        'base_url': 'http://localhost:11434/v1' ,\n",
        "    },\n",
        "]\n",
        "\n",
        "CEREBRAS_SMALL_CONFIG_LIST = {\"config_list\": cerebras_config_list}\n",
        "GROQ_SMALL_CONFIG_LIST = {\"config_list\": groq_small_config_list}\n",
        "CONFIG_LIST = {\"config_list\": oai_config_list}\n",
        "GEMINI_CONFIG_LIST = {\"config_list\": gemini_config_list}\n",
        "OAI_CONFIG_LIST = {\"config_list\": oai_config_list}\n",
        "CEREBRAS_CONFIG_LIST = {\"config_list\": cerebras_config_list}\n",
        "GROQ_CONFIG_LIST = {\"config_list\": groq_config_list}\n",
        "ANTHROPIC_CONFIG_LIST = {\"config_list\": anthropic_config_list}\n",
        "OLLAMA_CONFIG_LIST = {\"config_list\": ollama_config_list}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Config\n",
        "\n",
        "llm_config = GEMINI_CONFIG_LIST\n",
        "#llm_config = OAI_CONFIG_LIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### `agents + tools`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### user_proxy, executor, job_assistant, sql_assistant, reviewer_assistant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Agents\n",
        "\n",
        "user_proxy = autogen.UserProxyAgent(\n",
        "    name=\"Admin_User\",\n",
        "    system_message=\"A human admin. Interact with the assistants to complete the tasks. A common workflow is identifying a connection name, then identifying a schema, then identifying a table, then running a dq job, then checking the results.\",\n",
        "    description=\"\"\"A team member that wants to efficiently complete tasks.\"\"\",\n",
        "    code_execution_config=False,\n",
        "    human_input_mode=\"NEVER\",\n",
        ")\n",
        "\n",
        "executor = autogen.UserProxyAgent(\n",
        "    name=\"Executor_User\",\n",
        "    system_message=\"Executor. Execute the instructions and tools suggested by sql_assistant or job_assistant and report the results.\",\n",
        "    description=\"\"\"A computer terminal that performs no other action than running tool calls from sql_assistant or job_assistant.\"\"\",\n",
        "    human_input_mode=\"NEVER\",\n",
        "    llm_config=llm_config,\n",
        "    code_execution_config=False,  \n",
        ")\n",
        "\n",
        "job_assistant = autogen.AssistantAgent(\n",
        "    name=\"Job_Assistant\",\n",
        "    llm_config=llm_config,\n",
        "    system_message=\"\"\"Job Assistant. You are able to run dq jobs and check dq job results/status. \n",
        "You use the run_dq_job function to run dq jobs. You use the get_job_status function to check the results, one time each time you're asked.\n",
        "Because these dq job functions trigger an API call, the user will want current (up to date) results, you should execute the functions rather than rely on the historical context. \n",
        "Even if job was recently run, you can run it one more time if the user requests this. \n",
        "Example: run a dq job for tables w/ 'xyz' in the name (run dq jobs, you need a connection_name, dataset, query)\n",
        "Example: whats the status of the dq jobs (check dq job results/status, no arguments needed)\n",
        "Include the word 'TERMINATE' and summarize the answer, if you can answer the question from the context, rather than asking for more tasks.\"\"\",\n",
        ")\n",
        "\n",
        "sql_assistant = autogen.AssistantAgent(\n",
        "    name=\"SQL_Assistant\",\n",
        "    llm_config=llm_config,\n",
        "    system_message=\"\"\"SQL Assistant. You provide instructions to the executor to run a query on the 'metadata' table, in order to answer the most user questions.\n",
        "REMEMBER: \n",
        "- Only query the 'metadata' table.\n",
        "- Only use the columns 'connection_name', 'schema_name', 'table_name'\n",
        "- 'connections' typically refers to the 'connection_name' column\n",
        "- 'schema' typically refers to the 'schema_name' column\n",
        "- 'table' typically refers to the 'table_name' column\n",
        "- single quote for escape characters\n",
        "- focus on the immediate task, don't deviate from the task\n",
        "If the question is unclear, try a distinct or a limit 10 query to narrow down the results. \n",
        "If you're unsure, you can try 1 attempt to interpret the question (best guess). \n",
        "If you're still not sure, or there are no results after trying a query, ask for clarificaiton.\n",
        "EXAMPLE_PROMPT: use sql, count total number of tables in this schema\n",
        "EXAMPLE QUERY: select * from metadata where connection_name = '<CONNECTION_NAME>' and schema_name = '<SCHEMA>' and table_name like '%<SEARCH_STRING>%' limit 30\n",
        "\n",
        "IMPORTANT: If you can clearly answer the question, include the word 'TERMINATE' and summarize the answer, don't respond with the same query and don't ask to help with more tasks.\"\"\",\n",
        "    max_consecutive_auto_reply=4,\n",
        ")\n",
        "\n",
        "reviewer_assistant = autogen.AssistantAgent(\n",
        "    name=\"Reviewer_Assistant\",\n",
        "    system_message=\"\"\"Review the prompt and results to concisely answer the question. Summarize the initial question and answer so it's easy to understand. \n",
        "Use bullet points or lists if the sql or job assistant returns multiple results.\n",
        "As the reviewer assisant, you do not include actions, arguments, or tool calls. Only the planner_assistant includes that information.\n",
        "Include the word 'TERMINATE' with the summarized response, rather than asking for more tasks.\"\"\",\n",
        "    llm_config=llm_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### next steps assistant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [],
      "source": [
        "next_steps_agent = autogen.ConversableAgent(\n",
        "    name=\"next_steps_agent\",\n",
        "    system_message=\"\"\"You review conversation snippets to identify 2 key pieces of information and suggest actions for a user.\n",
        "\n",
        "Because the workflow follows a hierarchy of connection > schema > table > job / dataset, you should provide what most recent context is inferred, likely next steps, and missing context.\n",
        "Respond concisely, like the examples below to help guide the user. \n",
        "\n",
        "Example Responses:\n",
        "    # Example 0, No elements in context.\n",
        "    Based on this recent conversation history.\n",
        "    The most recently mentioned context includes:\n",
        "     - connection_name = none\n",
        "     - schema_name = none \n",
        "     - next steps: list connections, get job results\n",
        "     Note: These actions can be run at any point in the workflow. As for getting started, the most likely first step is to list distinct available connections.\n",
        "\n",
        "    # Example 1, Only connection_name recently mentioned.\n",
        "    Based on this recent conversation history.\n",
        "    The most recently mentioned context includes:\n",
        "     - connection_name = 'xyz' \n",
        "     - schema_name = none \n",
        "     - next steps: search for a schema, count tables in each schema\n",
        "     Note: Once you've identified a connection_name, the most common next step is to find a schema or analyze schemas in a connection.\n",
        "\n",
        "    # Example 2, connection_name and schema_name recently mentioned.\n",
        "    Based on this recent conversation history.\n",
        "    The most recently mentioned context includes:\n",
        "     - connection_name = 'xyz' \n",
        "     - schema_name = 'xyz' \n",
        "     - next steps: search for table(s)\n",
        "     Note: Once you've identified a connection_name and schema_name combination, the most common next step is to search for tables in a schema.\n",
        "\n",
        "    # Example 3, table_name(s) recently mentioned.\n",
        "    Based on this recent conversation history.\n",
        "    The most recently mentioned context includes:\n",
        "     - connection_name = none\n",
        "     - schema_name = none\n",
        "     - table_name(s) = claims, nyse \n",
        "     - next steps: run a dq job for one or several tables\n",
        "     Note: Once you've identified a table_name or several table_names, the most common next step is to run a dq job for one or several tables.\n",
        "\n",
        "    # Example 4, recent jobs have been run.\n",
        "    Based on this recent conversation history.\n",
        "    The most recently mentioned context includes:\n",
        "     - connection_name = xyz\n",
        "     - schema_name = xyz\n",
        "     - table_name(s) = xyz\n",
        "     - dataset(s) = xyz\n",
        "     - next steps: check job results\n",
        "     Note: Once you've run a job, the most common next step is to check the results of a dq job. Or you can always run the job again or start a new search.\n",
        "\n",
        "    While this is a simple workflow, you can always start a new search or run a job again. The user may not work linearly.\n",
        "      \"\"\",\n",
        "    llm_config=OAI_CONFIG_LIST,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### planner and context assistants (not needed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [],
      "source": [
        "planner_assistant = autogen.AssistantAgent(\n",
        "    name=\"Planner_Assistant\",\n",
        "    llm_config=llm_config,\n",
        "    system_message=\"\"\"You're an expert at finding the information you need from the context or guiding the user on additional information. \n",
        "First check if the answer is found in the context, for key details.\n",
        "If you can't find the information in the chat context, then respond with the key details and action, depending on the type of task.\n",
        "\n",
        "# Answer found in the context (no action needed)\n",
        "Example: \n",
        "- prompt: whats the 'xyz' connection name?\n",
        "- context: 'the connection name 'xyz' was found in the recent chat context.'  \n",
        "\n",
        "# Answer found in the context\n",
        "Example: \n",
        "- prompt: whats the 'xyz' connection name?\n",
        "- task: sql \n",
        "- action: look-up distinct connection_names in metadata table\n",
        "- arguments: none \n",
        "\n",
        "# Answer not found in the context, send to assistant to get more information\n",
        "Example:\n",
        "- prompt: what schemas are in the bigquery connection?\n",
        "- task: sql \n",
        "- action: look-up connection_names in metadata table\n",
        "- arguments: none \n",
        "\n",
        "# Send to jobassistant to perform this task\n",
        "Example:\n",
        "- prompt: whats the latest job results?\n",
        "- task: job status  \n",
        "- action: look-up job_status\n",
        "- arguments: none \n",
        "\n",
        "# Answer not found in the context, send to sql assistant to get more information\n",
        "Example:\n",
        "- prompt: what schemas are in the bigquery connection?\n",
        "- task: sql \n",
        "- action: look-up schema_names, by connection_name in metadata table\n",
        "- arguments: connection_name \n",
        "\n",
        "# Answer not found in the context, send to sql assistant to get more information\n",
        "Example:\n",
        "- prompt: list tables w/ 'xyz' in the name\n",
        "- task: sql \n",
        "- action: look-up table_names, by connection_name and schema_name in metadata table\n",
        "- arguments: connection_name, schema_name\n",
        "\n",
        "# Send to job assistant to perform this task\n",
        "Example:\n",
        "- prompt: run a dq job for tables w/ 'xyz' in the name, in the 'xyz' schema, use the previous connection\n",
        "- task: run job  \n",
        "- action: run a dq job for a table, include connection_name and schema_name\n",
        "- arguments: connection_name, schema_name, table_name\n",
        "\n",
        "These are examples of the initial analysis\n",
        "When a generic technology is mentioned, it typically just refers to a connection_name, try to find a connection_name \n",
        "If you can answer the question from the chat history, go ahead and answer the question, and include the word 'TERMINATE'.\n",
        "If you should send the question to the sql or job assistant, respond accordingly with the task, action, and arguments\n",
        "Or if you can't find the information you need, ask for clarification.\n",
        "NEVER: guess the answer, or make up an answer. use facts from the chat history or send to the sql or job assistant.\"\"\"\n",
        ")\n",
        "\n",
        "context_agent = autogen.ConversableAgent(\n",
        "    name=\"context_agent\",\n",
        "    system_message=\"\"\"You never include the word 'TERMINATE' in your response. \n",
        "You review conversation snippets to identify 2 key pieces of information for the task at hand.\n",
        "\n",
        "Because the workflow follows a hierarchy of connection > schema > table > job / dataset, you must identify key elements from the most recent context.\n",
        "Respond concisely. \n",
        "\n",
        "Example Responses:\n",
        "    # Example 0, No elements in context or not context at all..\n",
        "    Based on this recent conversation history.\n",
        "    The most recently mentioned context includes:\n",
        "     - connection_name = none\n",
        "     - schema_name = none \n",
        "     - next steps: list distinct connections, or maybe get job results\n",
        "     Note: The most common first step is to list distinct available connections.\n",
        "\n",
        "    # Example 1, Only connection_name recently mentioned.\n",
        "    Based on this recent conversation history.\n",
        "    The most recently mentioned context includes:\n",
        "     - connection_name = 'xyz' \n",
        "     - schema_name = none \n",
        "     - next steps: search for a schema, count tables in each schema\n",
        "     Note: Once you've identified a connection_name, a common next step is to search for a schema (exact or substring of the name).\n",
        "\n",
        "    # Example 2, connection_name and schema_name recently mentioned.\n",
        "    Based on this recent conversation history.\n",
        "    The most recently mentioned context includes:\n",
        "     - connection_name = 'xyz' \n",
        "     - schema_name = 'xyz' \n",
        "     - next steps: search for table(s)\n",
        "     Note: Once you've identified a connection_name and schema_name combination, a next step is to search for tables (exact or substring of the name) in a schema.\n",
        "\n",
        "    # Example 3, table_name(s) recently mentioned.\n",
        "    Based on this recent conversation history.\n",
        "    The most recently mentioned context includes:\n",
        "     - connection_name = none\n",
        "     - schema_name = none\n",
        "     - table_name(s) = claims, nyse \n",
        "     - next steps: run a dq job for one or several tables\n",
        "     Note: Once you've identified a table_name or several table_names, a common next step is to run a dq job for one or several tables.\n",
        "\n",
        "    # Example 4, recent jobs have been run.\n",
        "    Based on this recent conversation history.\n",
        "    The most recently mentioned context includes:\n",
        "     - connection_name = xyz\n",
        "     - schema_name = xyz\n",
        "     - table_name(s) = xyz\n",
        "     - dataset(s) = xyz\n",
        "     - next steps: check job results\n",
        "     Note: Once you've run a job, a common next step is to check the results of a dq job.\n",
        "      \"\"\",\n",
        "    llm_config=llm_config,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tools\n",
        "@executor.register_for_execution()\n",
        "@executor.register_for_llm()\n",
        "@job_assistant.register_for_llm(description=f\"\"\"Submits a DQ Job to run a DQ check.\n",
        "IMPORTANT: This requires a connection_name, dataset, query.\n",
        "NOTE: Use the table name as the dataset name\n",
        "The DQ Job query should use schema.table format and have a limit 10000 to always limit results. \n",
        "\"\"\")\n",
        "def run_dq_job(dataset: Annotated[str, \"dataset name to use\"], query: Annotated[str, \"query to use\"], connection_name: Annotated[str, \"connection name to use\"]):\n",
        "    import requests\n",
        "    import json \n",
        "    from datetime import datetime\n",
        "    import urllib3\n",
        "    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
        "\n",
        "    row_count = 0 \n",
        "    try:\n",
        "        import duckdb \n",
        "        conn = duckdb.connect(database=':memory:')\n",
        "        conn.sql(f\"create table metadata as select * from read_csv_auto('/tmp/snippets/connection_schema.csv')\")\n",
        "        table_name = dataset\n",
        "        schema_name = schema_name = query.split(' ')[3].split('.')[0].strip()\n",
        "        print(schema_name)\n",
        "        sql_statement = f\"select * from metadata where connection_name = '{connection_name}' and schema_name = '{schema_name}' and table_name = '{table_name}' limit 1\"\n",
        "        rs = conn.sql(sql_statement.replace(\"\\\\\",\"\"))\n",
        "        print(rs)\n",
        "        print(rs.to_df().shape)\n",
        "        row_count = rs.to_df().shape[0]\n",
        "        print(f\"row_count: {row_count}\")\n",
        "        if row_count == 0:\n",
        "            return f\"Unable to find table {table_name} in schema {schema_name} for connection {connection_name}. Please confirm the schema and connection and try again.\"\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return f\"Unable to validate and table {table_name} in schema {schema_name} for connection {connection_name}. Please confirm the schema and connection and try again.\"\n",
        "\n",
        "    def get_api_token():\n",
        "        prod_auth = requests.post(os.environ.get(\"DQ_URL\") + '/v3/auth/signin',\n",
        "                            headers={'Accept-Language': 'en-US,en;q=0.9', 'Connection': 'keep-alive', 'Content-Type': 'application/json'},\n",
        "                            data=json.dumps({'username': os.environ.get(\"DQ_USERNAME\"), 'password': os.environ.get(\"DQ_CREDENTIAL\"), 'iss': os.environ.get(\"DQ_TENANT\")}),\n",
        "                                verify=False)\n",
        "        token = prod_auth.json()[\"token\"]\n",
        "        return token\n",
        "\n",
        "    token = get_api_token()\n",
        "    headers = {'Authorization': 'Bearer ' + token}\n",
        "    run_id = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "    params = {'dataset': \"AI_\"+dataset, \n",
        "              'runId': run_id, \n",
        "              'pushdown': {'connectionName': connection_name, \n",
        "                           'sourceQuery': query.replace(\"\\\\\", \"\") },\n",
        "              'agentId': {'id': 0}\n",
        "              }\n",
        "    response = requests.request('POST', os.environ.get(\"DQ_URL\") + '/v2/run-job-json', headers=headers, json=params, verify=False)\n",
        "    if response.status_code == 200:\n",
        "        print(response.json())\n",
        "    return f\"job triggered successfully, for {table_name} in schema {schema_name} for connection {connection_name}. response: {query} \\n\\n {response.json()}\"\n",
        "\n",
        "@executor.register_for_execution()\n",
        "@executor.register_for_llm()\n",
        "@job_assistant.register_for_llm(description=f\"\"\"check status\"\"\")\n",
        "def get_job_status():\n",
        "    import requests\n",
        "    import json\n",
        "    import urllib3\n",
        "    import pandas as pd\n",
        "    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
        "\n",
        "    def get_api_token():\n",
        "        prod_auth = requests.post(os.environ.get(\"DQ_URL\") + '/v3/auth/signin',\n",
        "                            headers={'Accept-Language': 'en-US,en;q=0.9', 'Connection': 'keep-alive', 'Content-Type': 'application/json'},\n",
        "                            data=json.dumps({'username': os.environ.get(\"DQ_USERNAME\"), 'password': os.environ.get(\"DQ_CREDENTIAL\"), 'iss': os.environ.get(\"DQ_TENANT\")}),\n",
        "                            verify=False)\n",
        "        token = prod_auth.json()[\"token\"]\n",
        "        return token\n",
        "\n",
        "    token = get_api_token()\n",
        "    headers = {'Authorization': 'Bearer ' + token}\n",
        "    params = {\n",
        "        'jobStatus': '',\n",
        "        'limit': '5',\n",
        "    }\n",
        "\n",
        "    response = requests.get(os.environ.get(\"DQ_URL\") + '/v2/getowlcheckq', params=params, headers=headers, verify=False)\n",
        "\n",
        "    #df = pd.DataFrame(response.json()['data'], columns=['dataset', 'runId', 'status', 'activity', 'updtTs'])\n",
        "    df = pd.DataFrame(response.json()['data'], columns=['dataset', 'status', 'activity'])\n",
        "    return f\"job status: ~~~{df.to_markdown(index=False, tablefmt='presto', floatfmt='.0%').replace('000','')}~~~\"\n",
        "\n",
        "@executor.register_for_execution()\n",
        "@executor.register_for_llm()\n",
        "@sql_assistant.register_for_llm(description=f\"\"\"Runs A SELECT statement on 'metadata' table. the columns 'connection_name', 'schema_name', 'table_name'\"\"\")\n",
        "def run_sql_statement(sql_statement: Annotated[str, \"SQL statement to execute\"]):\n",
        "    import duckdb \n",
        "    conn = duckdb.connect(database=':memory:')\n",
        "    conn.sql(f\"create table metadata as select * from read_csv_auto('/tmp/snippets/connection_schema.csv')\")\n",
        "    rs = conn.sql(sql_statement.replace(\"\\\\\",\"\"))\n",
        "    rs.show()\n",
        "    return f\"results: ~~~{sql_statement} \\n {rs.to_df().drop_duplicates().head(15).to_markdown(index=False)}~~~\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transform\n",
        "\n",
        "from autogen.agentchat.contrib.capabilities import transform_messages, transforms\n",
        "import json\n",
        "import re\n",
        "pattern = r\"~~~.*?~~~\"\n",
        "import pprint\n",
        "import copy\n",
        "import re\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "class MessageRedact:\n",
        "    def __init__(self):\n",
        "        self._content_wrapper_pattern = r\"~~~.*?~~~\" \n",
        "        #self._openai_key_pattern = r\"sk-([a-zA-Z0-9]{48})\"\n",
        "        self._replacement_string = \"TRUNCATED_MESSAGE\"\n",
        "\n",
        "    def apply_transform(self, messages: List[Dict]) -> List[Dict]:\n",
        "        temp_messages = copy.deepcopy(messages)\n",
        "\n",
        "        total_tool_calls = 0\n",
        "        total_tool_responses = 0\n",
        "        counter = 0\n",
        "        for m in temp_messages:\n",
        "            #print(m['content'])\n",
        "            if \"tool_calls\" in m:\n",
        "                total_tool_calls += len(m[\"tool_calls\"])\n",
        "            if \"tool_responses\" in m:\n",
        "                total_tool_responses += len(m[\"tool_responses\"])\n",
        "                print(f\"total_tool_calls: {total_tool_calls}, total_tool_responses: {total_tool_responses}\")\n",
        "            \n",
        "            content_string = \"Context from previous tool calls: \"\n",
        "            if \"tool_calls\" in m:\n",
        "                for i in m[\"tool_calls\"]:\n",
        "                    # if 'id' in i:\n",
        "                    #     i.pop('id')\n",
        "\n",
        "                    # json dump tool call to string\n",
        "                    content_string += \"\\n function name: \" + i['function']['name']\n",
        "                    content_string += \"\\n function arguments: \" + json.dumps(i['function']['arguments'])\n",
        "                    # content_string += \"\\n function arguments: \"\n",
        "                    # tool_args = json.loads(i['function']['arguments'])\n",
        "                    # for k, v in tool_args.items():\n",
        "                    #     content_string += f\"\\n  - {k}: {v}\"\n",
        "                \n",
        "                m.pop(\"tool_calls\")\n",
        "                m['role'] = \"user\"\n",
        "                m[\"content\"] = content_string\n",
        "\n",
        "            if \"tool_responses\" in m:\n",
        "                for j in m[\"tool_responses\"]: \n",
        "                    # if 'tool_call_id' in j:\n",
        "                    #     j.pop('tool_call_id')\n",
        "                    \n",
        "                    # json dump tool response to string\n",
        "\n",
        "                    content_string += \"Context from previous tool response:\" + json.dumps(j['content'])\n",
        "                    \n",
        "                m[\"content\"] = content_string\n",
        "                m['role'] = \"user\"\n",
        "                m.pop(\"tool_responses\")  \n",
        "                \n",
        "            message = m\n",
        "            \n",
        "            if isinstance(message[\"content\"], str):\n",
        "                counter += 1\n",
        "                if counter < 5:\n",
        "                    message[\"content\"] = message[\"content\"][:600]\n",
        "                else:\n",
        "                    message[\"content\"] = message[\"content\"][:1000]\n",
        "                    #message[\"content\"] = re.sub(self._content_wrapper_pattern, self._replacement_string, message[\"content\"],  flags=re.DOTALL )  \n",
        "            \n",
        "                if \"tool_responses\" in message:\n",
        "                    if isinstance(message[\"tool_responses\"], list):\n",
        "                        for item in message[\"tool_responses\"]:\n",
        "                            if isinstance(item[\"content\"], str):\n",
        "                                item[\"content\"] = re.sub(self._content_wrapper_pattern, self._replacement_string, item[\"content\"], flags=re.DOTALL)\n",
        "                                \n",
        "            elif isinstance(message[\"content\"], list):\n",
        "                for item in message[\"content\"]:\n",
        "                    if item[\"type\"] == \"text\":\n",
        "                        item[\"text\"] = re.sub(self._content_wrapper_pattern, self._replacement_string, item[\"text\"], flags=re.DOTALL)\n",
        "\n",
        "        temp_messages.append({'content': 'Thank you. I will review this information and get back to you.', 'role': 'user', 'name': 'Admin_User'})\n",
        "        return temp_messages\n",
        "\n",
        "    def get_logs(self, pre_transform_messages: List[Dict], post_transform_messages: List[Dict]) -> Tuple[str, bool]:\n",
        "        keys_redacted = self._count_redacted(post_transform_messages) - self._count_redacted(pre_transform_messages)\n",
        "        if keys_redacted > 0:\n",
        "            return f\"Redacted {keys_redacted} Matching Patterns.\", True\n",
        "        return \"\", False\n",
        "\n",
        "    def _count_redacted(self, messages: List[Dict]) -> int:\n",
        "        # counts occurrences in message content\n",
        "        count = 0\n",
        "        for message in messages:\n",
        "            if isinstance(message[\"content\"], str):\n",
        "                if self._replacement_string in message[\"content\"]:\n",
        "                    count += 1\n",
        "            elif isinstance(message[\"content\"], list):\n",
        "                for item in message[\"content\"]:\n",
        "                    if isinstance(item, dict) and \"text\" in item:\n",
        "                        if self._replacement_string in item[\"text\"]:\n",
        "                            count += 1\n",
        "        return count\n",
        "\n",
        "# redact_handling = transform_messages.TransformMessages(transforms=[MessageRedact()])\n",
        "# redact_handling.add_to_agent(user_proxy)\n",
        "\n",
        "# Limit the message history to the 3 most recent messages\n",
        "# context_handling = transform_messages.TransformMessages(\n",
        "#     transforms=[\n",
        "#         transforms.MessageHistoryLimiter(max_messages=10),        \n",
        "#     ]\n",
        "# )\n",
        "# context_handling.add_to_agent(user_proxy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### `prompts`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [],
      "source": [
        "#prompt = \"\"\"list the distinct connections\"\"\" \n",
        "#prompt = \"list distinct connections\" \n",
        "#prompt = \"whats the bigquery connection name?\"\n",
        "#prompt = \"is there a samples schema in the bigquery connection?\"\n",
        "#prompt = \"list the distinct schemas in the bigquery connection\"\n",
        "#prompt = \"count the distinct tables in the bigquery connection\"\n",
        "\n",
        "#prompt = \"\"\"\n",
        "# in that bigquery connection\n",
        "# in the same schema\n",
        "# list tables w/ 'nyse' in the name\n",
        "# \"\"\"\n",
        "\n",
        "#prompt=\"\"\"run a job for those 3 tables\"\"\"\n",
        "#prompt=\"can you run a job for those tables (the first 4 listed)?\"\n",
        "#prompt = \"list the tables listed w/ claims in the name, use bigquery connection, samples schema\"\n",
        "#prompt = \"run a dq job for the claims dents and claims detail table\" \n",
        "#prompt = \"run a dq job for the all the claims tables listed in that connection and schema\" \n",
        "\n",
        "#prompt=\"\"\"what schema am i using?\"\"\"\n",
        "#prompt=\"\"\"get the latest job results\"\"\"\n",
        "#prompt=\"\"\"what connections do i have?\"\"\"\n",
        "#prompt=\"\"\"what schemas are in that bigquery connection?\"\"\"\n",
        "\n",
        "#prompt = \"in the sql server connection, what tables are in the example schema with geo_uip_stat in the name\"\n",
        "#prompt = \"are there tables with 'customer' in the name, in the tf_esg schema in the snowflake connection?\"\n",
        "#prompt = \"using the connecton named bigquery and the samples schema, what tables have census in the name?\"\n",
        "#prompt = \"do all upper case, in the snowflake connection not sap, what tables are in the 'PUBLIC' schema with CUSTOMER in the name\"\n",
        "#prompt = \"run a job for those CUSTOMER tables listed\"\n",
        "#prompt = \"check the sql server connection, not sap, what tables are in the 'dbo' schema with accounts as part of the name?\"\n",
        "#prompt = \"check job status\"\n",
        "#prompt = \"check the samples schema, in the bigquery connection, tables with 'claims' in he table name. \"\n",
        "#prompt = \"run a  job, use the samples schema, in the bigquery connection, run a job the last 3 listed w/ with 'claims_dent' in the table name. \"\n",
        "#prompt = \"run a job for the claims_dent table listd, same bigquery connection\" \n",
        "#prompt = \"list the available connection names\"\n",
        "#prompt = \"can you get a dq job results? \"\n",
        "#prompt = \"run a dq job for the loan_customer tables, bigquery connection, samples schema\"\n",
        "#prompt = \"run a dq job for hte samples schema, bigquery connection, claims_dent in the name\"\n",
        "#prompt = \"list the distinct schemas in the bigquery connection\"\n",
        "#prompt = \"list distinct connections\" \n",
        "#prompt = \"check job status\"\n",
        "\n",
        "prompt = \"\"\" run a job for these items\n",
        "- in the samples schema, in the bigquery connection, tables with 'claims_dents_4' and claim details in the name. \n",
        "\"\"\"\n",
        "#prompt = \"whats the bigquery connection name?\"\n",
        "#prompt = \"whats the status of that customer job\"\n",
        "#prompt = \"list claims tables, samples schema, use the bigquery connection\"\n",
        "#prompt = \"what were the names of the tables were just mentioned?\"\n",
        "#prompt = \"what was the bigquery connection and schema mentioned?\"\n",
        "#prompt = \"list the tables w/ patent in the name, use bigquery connection, samples schema\"\n",
        "#prompt = \"run a job for those tables just listed.\"\n",
        "\n",
        "prompt = \"list the tables listed w/ claims in the name, use bigquery connection, samples schema\"\n",
        "#prompt = \"list the tables listed w/ claims in the name, use previous connection and schema\" \n",
        "#prompt = \"list the tables listed w/ customer in the name, same connection and schema\" \n",
        "#prompt = \"list the tables listed w/ patent in the name, same connection and schema\" \n",
        "#prompt = \"list the tables listed w/ claims in the name, use bigquery connection, samples schema\"\n",
        "\n",
        "prompt = \"run a dq job for the claims dents and claims detail table\" \n",
        "#prompt = \"what tables did you last mention? \"\n",
        "#prompt = \"run a dq job for those tables\"\n",
        "#prompt = \"run a dq job for those tables w/ customer, use bigquery connection, samples schema\"\n",
        "#prompt = \"is there a  bigquery connection?\"\n",
        "#prompt = \"whats the name of the bigquery connection?\"\n",
        "#prompt = \"what connections are there?\"\n",
        "#prompt = \"what schemas are in that bigquery connection\"\n",
        "#prompt = \"use the samples schema, any tables that start with 'm'\"\n",
        "\n",
        "prompt = \"is there a samples schema in the bigquery connection?\"\n",
        "#prompt = \"are there tables taht start with 'g' in that schema? \"\n",
        "#prompt = \"run a job for global_air_quality table \"\n",
        "#prompt = \"what are the latest results?\"\n",
        "# prompt = \"are there tables taht start with 'k' in that schema? \"\n",
        "# prompt = \"\"\"run a job for those tables listed\n",
        "# | APPROVED_BIGQUERY_PUSHDOWN | samples       | kb_participant_loc    |\n",
        "# | APPROVED_BIGQUERY_PUSHDOWN | samples       | kishore               |\n",
        "# | APPROVED_BIGQUERY_PUSHDOWN | samples       | kb_participant_shirt  |\"\"\"\n",
        "\n",
        "#prompt = \"what tables did you last mention? \"\n",
        "#prompt = \"check job status\"\n",
        "\n",
        "# connection\n",
        "prompt=\"list the distinct connections available\"\n",
        "prompt=\"\"\"what connections do i have?\"\"\"\n",
        "prompt = \"whats the bigquery connection name?\"\n",
        "prompt=\"\"\"whats the name of the bigquery connection? it's not named bigquery, check for more info\"\"\"\n",
        "\n",
        "# schema\n",
        "prompt=\"\"\"what schemas are in that bigquery connection?\"\"\"\n",
        "prompt=\"\"\"what schema was i using? \"\"\"\n",
        "prompt=\"is there a schema w/ 'samples' in the name in that connection\"\n",
        "prompt=\"\"\"what connection was i just using? \"\"\"\n",
        "\n",
        "# table\n",
        "prompt=\"\"\"\n",
        "in that bigquery connection\n",
        "in the same schema\n",
        "list tables w/ 'nyse' in the name\n",
        "\"\"\"\n",
        "prompt=\"list first 5 tables that start with the letter 'd' \"  \n",
        "\n",
        "# job\n",
        "prompt=\"\"\"run a job for those tables just mentioned\"\"\"\n",
        "prompt=\"\"\"\n",
        "in the connection APPROVED_BIGQUERY_PUSHDOWN\n",
        "in the 'samples' schema\n",
        "run a dq job for those 5 tables\n",
        "\"\"\"\n",
        "\n",
        "# results\n",
        "prompt=\"\"\"get the latest job results\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### `chat`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### groupchat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Groupchat \n",
        "allowed_transitions = {\n",
        "    user_proxy: [sql_assistant, job_assistant, executor, reviewer_assistant],\n",
        "    sql_assistant: [executor, user_proxy],\n",
        "    job_assistant: [executor, user_proxy],\n",
        "    executor: [reviewer_assistant, user_proxy],\n",
        "    reviewer_assistant: [user_proxy],\n",
        "}\n",
        "\n",
        "groupchat = autogen.GroupChat(\n",
        "    agents=[user_proxy, sql_assistant, job_assistant, executor, reviewer_assistant], \n",
        "    messages=[], max_round=5,\n",
        "    speaker_transitions_type=\"allowed\", \n",
        "    allowed_or_disallowed_speaker_transitions=allowed_transitions,\n",
        "    send_introductions=True\n",
        ")\n",
        "\n",
        "manager = autogen.GroupChatManager(groupchat=groupchat, name=\"manager\", llm_config=llm_config, is_termination_msg=lambda x: \"TERMINATE\" in x.get(\"content\", \"\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [],
      "source": [
        "# functions for chat and recent history\n",
        "\n",
        "def chat(manager, prompt, groupchat):\n",
        "    clean_transform = MessageRedact()\n",
        "    tmp_messages = copy.deepcopy(manager.groupchat.messages)\n",
        "    end_window = len(tmp_messages)\n",
        "    start_window = 0\n",
        "    retain_messages = 5\n",
        "\n",
        "    intro_message = {'content': groupchat.introductions_msg(), 'role': 'user', 'name': 'manager'}\n",
        "    intro_string = 'We have assembled a great team today'\n",
        "\n",
        "    for m in tmp_messages:\n",
        "        if intro_string in m['content']:\n",
        "            tmp_messages.remove(m)\n",
        "\n",
        "    if len(tmp_messages) < retain_messages:\n",
        "        start_window = 0\n",
        "    else:\n",
        "        start_window = len(tmp_messages) - retain_messages\n",
        "\n",
        "    processed_messages = clean_transform.apply_transform(tmp_messages[start_window:end_window])\n",
        "    processed_messages.insert(0, intro_message)\n",
        "\n",
        "    last_agent, last_message = manager.resume(messages=processed_messages)\n",
        "    chat_result = user_proxy.initiate_chat(recipient=manager, message=prompt, clear_history=False, max_rounds=7)\n",
        "    return chat_result, manager \n",
        "\n",
        "def get_recent_context(manager):\n",
        "    recent_conversation = \"\"\n",
        "    for m in manager.groupchat.messages:\n",
        "        if m['content'] != '' and \"Hello everyone.\" not in m['content']:\n",
        "            recent_conversation += m['content']\n",
        "    return recent_conversation\n",
        "\n",
        "#sql_assistant.llm_config['config_list']\n",
        "#sql_assistant.llm_config[\"tools\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### initiate chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# initiate chat\n",
        "prompt=\"\"\"list the distinct connections\"\"\" \n",
        "\n",
        "result = user_proxy.initiate_chat(\n",
        "manager,\n",
        "message=prompt,\n",
        "clear_history=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### resume chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "in the connection APPROVED_BIGQUERY_PUSHDOWN\n",
        "check the 'samples' schema\n",
        "list first 3 tables w/ 'census' in the name\n",
        "\"\"\"\n",
        "\n",
        "cr, manager = chat(manager, prompt, groupchat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### post chat, next steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "instruction_msg = \"\"\"Given this conversation snippet below, review and analyze and respond with most recent context. Include the names of the specific elements mentioned.\n",
        "If multiple items are mentioned, respond with the most recent context i.e. towards the bottom. \n",
        "\n",
        "Conversation:\n",
        "\"\"\"\n",
        "prompt = \"\"\"\n",
        "in the connection APPROVED_BIGQUERY_PUSHDOWN\n",
        "in the 'samples' schema\n",
        "list tables with 'claims' in the name\n",
        "\"\"\"\n",
        "\n",
        "cr, manager = chat(manager, prompt, groupchat)\n",
        "\n",
        "recent_conversation = get_recent_context(manager)\n",
        "instruction = f\"\"\"{instruction_msg}\n",
        "{recent_conversation}\"\"\"\n",
        "\n",
        "reply = next_steps_agent.generate_reply(messages=[{'content': instruction, 'role': 'user'}])\n",
        "print(reply)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### `chat with pre-routine actions look-up (RAG) + context `"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {},
      "outputs": [],
      "source": [
        "#CONFIG_LIST = OAI_CONFIG_LIST\n",
        "#CONFIG_LIST = GROQ_SMALL_CONFIG_LIST\n",
        "#CONFIG_LIST = CEREBRAS_SMALL_CONFIG_LIST\n",
        "#CONFIG_LIST = GROQ_CONFIG_LIST\n",
        "CONFIG_LIST = GEMINI_CONFIG_LIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [],
      "source": [
        "connection_parsing_prompt = \"\"\"You're an expert at identifying the connection name from the text.\n",
        "\n",
        "INSTRUCTIONS:\n",
        "You must respond with one of the following valid connection names: 'APPROVED_SAPHANA_PD', 'APPROVED_SNOWFLAKE_PUSHDOWN', 'APPROVED_BIGQUERY_PUSHDOWN', 'APPROVED_SQL_SERVER_PD', or 'NONE':\n",
        " - if text contains 'saphana', respond with the word 'APPROVED_SAPHANA_PD'\n",
        " - if text contains 'snowflake', respond with the word 'APPROVED_SNOWFLAKE_PUSHDOWN'\n",
        " - if text contains 'bigquery', respond with the word 'APPROVED_BIGQUERY_PUSHDOWN'\n",
        " - if text contains 'sql server', respond with 'APPROVED_SQL_SERVER_PD'\n",
        " - else if you're not sure, respond with 'NONE'\n",
        "\n",
        "IMPORTANT: Respond with 'NONE' if you're not sure, don't guess, do not make up names.\n",
        "ALWAYS: Respond with just one word\n",
        "ALWAYS: Respond with a valid connection name, only use your best guess from the text.\n",
        "\"\"\"\n",
        "\n",
        "connection_parsing_assistant = autogen.AssistantAgent(\n",
        "    name=\"Parsing_agent\",\n",
        "    human_input_mode=\"NEVER\",\n",
        "    max_consecutive_auto_reply=3,\n",
        "    llm_config=CONFIG_LIST,\n",
        "    system_message=connection_parsing_prompt,\n",
        "    code_execution_config=False\n",
        ")\n",
        "\n",
        "schema_parsing_prompt = \"\"\"You help the human identify the inferred SCHEMA name. \n",
        "You must parse out the name that represents the schema they want to use.\n",
        "\n",
        "You only care about finding the SCHEMA name\n",
        "IMPORTANT: Ignore connection entity references\n",
        "IMPORTANT: Ignore table entity references\n",
        "\n",
        "Always Respond with one word (the name you believe to be correct). \n",
        "\n",
        "Example: In the snowflake connection, using the public schema, what tables have xyz in the name.\n",
        "Response: public\n",
        "\n",
        "REMEMBER: Just one word answer, no explanations. \n",
        "ALWAYS: Respond with a valid schema name, only use your best guess from the message, do not make up names.\n",
        "Response with 'NA' if you're not sure, don't guess. \n",
        "\"\"\"\n",
        "\n",
        "schema_parsing_assistant = autogen.AssistantAgent(\n",
        "    name=\"Schema_Parsing_agent\",\n",
        "    llm_config=CONFIG_LIST,\n",
        "    system_message=schema_parsing_prompt,\n",
        "    max_consecutive_auto_reply=4,\n",
        "    code_execution_config=False\n",
        ")\n",
        "\n",
        "table_parsing_prompt = \"\"\"You help the human identify the inferred TABLE name. \n",
        "You must parse out the name that represents the table they want to use. It could be a substring of the table name.\n",
        "\n",
        "You only care about finding the TABLE name reference\n",
        "IMPORTANT: Ignore connection and schema entity references\n",
        "\n",
        "Always Respond with one word (the name you believe to be correct). \n",
        "\n",
        "Example: In the snowflake connection, using the public schema, what tables have 'abc' in the name.\n",
        "Response: abc\n",
        "\n",
        "REMEMBER: Just one word answer, no explanations. \n",
        "ALWAYS: Respond with a valid table name, only use your best guess from the message, do not make up names.\n",
        "IMPORTANT: Response with 'NA' if you're not sure, don't guess. \n",
        "\"\"\"\n",
        "\n",
        "table_parsing_assistant = autogen.AssistantAgent(\n",
        "    name=\"Table_Parsing_agent\",\n",
        "    llm_config=CONFIG_LIST,\n",
        "    system_message=table_parsing_prompt,\n",
        "    max_consecutive_auto_reply=4,\n",
        "    code_execution_config=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### pre-screen functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_sql(c, s, t, conn):\n",
        "    validated_c = \"\"\n",
        "    validated_s = \"\"\n",
        "    validated_t = \"\"\n",
        "    element_string = \"\"\n",
        "    return_string = \"\"\n",
        "    try:\n",
        "        if c != \"\":\n",
        "            connection_statement = f\"\"\"select distinct connection_name from metadata where lower(connection_name) = '{c.lower()}'\"\"\"\n",
        "            rs = conn.sql(connection_statement.replace(\"\\\\\",\"\"))\n",
        "            c_rc = rs.to_df().count().iloc[0]\n",
        "            \n",
        "            if c_rc > 0:\n",
        "                return_string = \"Previous SQL results history: \" + connection_statement + \"\\n\"\n",
        "                #return_string += \"Row Count: \" + str(c_rc) + \"\\n\"\n",
        "                return_string += rs.to_df().drop_duplicates().head(20).to_markdown(index=False)\n",
        "                validated_c = c\n",
        "            else:\n",
        "                print(\"No results found\")\n",
        "\n",
        "        if s != \"\" and c_rc > 0:\n",
        "            schema_statement = f\"\"\"select distinct schema_name from metadata where lower(connection_name) = '{c.lower()}' and lower(schema_name) = '{s.lower()}' \"\"\" \n",
        "            \n",
        "            rs = conn.sql(schema_statement.replace(\"\\\\\",\"\"))\n",
        "            s_rc = rs.to_df().count().iloc[0]\n",
        "            if s_rc > 0:\n",
        "                return_string = \"Previous SQL results history: \" + schema_statement + \"\\n\"\n",
        "                #return_string += \"Row Count: \" + str(s_rc) + \"\\n\"\n",
        "                return_string += rs.to_df().drop_duplicates().head(20).to_markdown(index=False)\n",
        "                validated_s = s\n",
        "            else:\n",
        "                print(\"No results found\")\n",
        "\n",
        "        if t != \"\" and c_rc > 0 and s_rc > 0:\n",
        "            \n",
        "            table_statement = f\"\"\"select * from metadata where lower(connection_name) = '{c.lower()}' and lower(schema_name) = '{s.lower()}' and lower(table_name) like '%{t.lower()}%' \"\"\" \n",
        "            rs = conn.sql(table_statement.replace(\"\\\\\",\"\"))\n",
        "            t_rc = rs.to_df().count().iloc[0]\n",
        "            if t_rc > 0:\n",
        "                return_string = \"Previous SQL results history: \" + table_statement + \"\\n\"\n",
        "                return_string += rs.to_df().drop_duplicates().head(20).to_markdown(index=False)\n",
        "                validated_t = t\n",
        "            else:\n",
        "                print(\"No results found\")\n",
        "    except: \n",
        "        pass\n",
        "    \n",
        "    return {\n",
        "        \"element_context\" : element_string, \n",
        "        \"sql_context\" : return_string,\n",
        "        \"validated_connection\" : validated_c,\n",
        "        \"validated_schema\" : validated_s,\n",
        "        \"validated_table\" : validated_t\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [],
      "source": [
        "def filter_step(prompt, suggested_task):\n",
        "    c=\"\"\n",
        "    s=\"\"\n",
        "    t=\"\" \n",
        "\n",
        "    filter_context = \"\"\n",
        "    sql_context = {}\n",
        "    validated_c = \"None\"\n",
        "    validated_s = \"None\"\n",
        "    validated_t = \"None\"\n",
        "    if 'sql' in suggested_task.strip():\n",
        "    \n",
        "        c_ai = connection_parsing_assistant.generate_reply(messages=[{\"role\" : \"user\", \"content\": prompt}])\n",
        "        s_ai = schema_parsing_assistant.generate_reply(messages=[{\"role\" : \"user\", \"content\": prompt}])\n",
        "        t_ai = table_parsing_assistant.generate_reply(messages=[{\"role\" : \"user\", \"content\": prompt}])\n",
        "        \n",
        "        print(c_ai)\n",
        "        print(s_ai)\n",
        "        print(t_ai)\n",
        "        for i in range(3):\n",
        "            \n",
        "            if None == c_ai:\n",
        "                c_ai = connection_parsing_assistant.generate_reply(messages=[{\"role\" : \"user\", \"content\": prompt}])\n",
        "            elif None != c_ai:\n",
        "                try:\n",
        "                    c=c_ai['content'].replace(\"\\n\", \"\").strip()\n",
        "                except:\n",
        "                    c=c_ai\n",
        "            \n",
        "            if None == s_ai:\n",
        "                s_ai = schema_parsing_assistant.generate_reply(messages=[{\"role\" : \"user\", \"content\": prompt}])\n",
        "            elif None != s_ai:\n",
        "                try:\n",
        "                    s=s_ai['content'].replace(\"\\n\", \"\").strip()\n",
        "                except:\n",
        "                    s=s_ai\n",
        "\n",
        "            if None == t_ai:\n",
        "                t_ai = table_parsing_assistant.generate_reply(messages=[{\"role\" : \"user\", \"content\": prompt}])\n",
        "            elif None != t_ai:\n",
        "                try:\n",
        "                    t=t_ai['content'].replace(\"\\n\", \"\").strip()\n",
        "                except:\n",
        "                    t=t_ai\n",
        "        \n",
        "        identified_connection, identified_schema, identified_table = c, s, t\n",
        "        identified_elements_string = f\"{c} > {s} > {t}\"\n",
        "        print(\"IDENTIFIED ELEMENTS: \", identified_elements_string)\n",
        "        # filter_context += \"\\nThese are potential identified elements:\\n \" + identified_elements_string\n",
        "        # filter_context += next_task[['lookup_values', 'similarities']].to_markdown(index=False ).replace(\"lookup_values\", \"tasks\")\n",
        "        #filter_context += \"\\nSimilar users have used the sql_assistant tool to help identify the connection, schema, and table\"\n",
        "        sql_context = test_sql(c, s, t, conn)\n",
        "        validated_c = sql_context.get(\"validated_connection\", \"\")\n",
        "        validated_s = sql_context.get(\"validated_schema\", \"\")\n",
        "        validated_t = sql_context.get(\"validated_table\", \"\")\n",
        "    else: \n",
        "        filter_context = \"You will likely want to either 'list distinct connections' to or request 'check job status\" \n",
        "        for i in range(1):\n",
        "            c_ai = connection_parsing_assistant.generate_reply(messages=[{\"role\" : \"user\", \"content\": prompt}])\n",
        "            print(c_ai)\n",
        "            \n",
        "            if None != c_ai:\n",
        "                try:\n",
        "                    c=c_ai['content'].replace(\"\\n\", \"\").strip()\n",
        "                except:\n",
        "                    c=c_ai\n",
        "            \n",
        "        sql_context = test_sql(c, s, t, conn)\n",
        "        validated_c = sql_context.get(\"validated_connection\", \"\")\n",
        "        # assisant_response = connection_parsing_assistant.generate_reply(messages=[{\"role\" : \"user\", \"content\": prompt}])\n",
        "        # print(assisant_response)\n",
        "        # if assisant_response is not None:\n",
        "        #     try:\n",
        "        #         filter_context += \"\\nAssistant response: \" + assisant_response['content'] + \"\\n\"\n",
        "        #     except:\n",
        "        #         filter_context += \"\\nAssistant response: \" + assisant_response + \"\\n\"\n",
        "        # else:\n",
        "        #     filter_context += \"\\nAssistant response: None\"\n",
        "        \n",
        "        # filter_context += \"Other similar tasks are:\\n\"\n",
        "        # print(next_task[['lookup_values']].to_markdown(index=False ))\n",
        "        # filter_context += next_task[['lookup_values']].to_markdown(index=False ).replace(\"lookup_values\", \"tasks\")\n",
        "        # print(next_task[['lookup_values', 'action', 'similarities']].to_markdown(index=False ))\n",
        "        \n",
        "    return {\n",
        "        \"filter_context\" : filter_context,\n",
        "        \"sql_context\" : sql_context,\n",
        "        \"connection_name\" : c,\n",
        "        \"schema_name\" : s,\n",
        "        \"table_name\" : t,\n",
        "        \"validated_connection\" : validated_c,\n",
        "        \"validated_schema\" : validated_s,\n",
        "        \"validated_table\" : validated_t\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {},
      "outputs": [],
      "source": [
        "def search_docs(df, user_query, top_n=4, to_print=True):\n",
        "    embedding = get_embedding(\n",
        "        user_query,\n",
        "        model=EMBEDDING_MODEL \n",
        ")\n",
        "    df[\"similarities\"] = df.saved_embedding.apply(lambda x: cosine_similarity(x, embedding))\n",
        "\n",
        "    res = (\n",
        "        df.sort_values(\"similarities\", ascending=False)\n",
        "        .head(top_n)\n",
        "    )\n",
        "    # if to_print:\n",
        "    #     display(res[['action', 'lookup_values', 'similarities']])\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_next_task(df, user_query, top_n=3, to_print=True):\n",
        "    s = search_docs(df, user_query, top_n=top_n, to_print=to_print)\n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_filter_context(suggested_task, filter_context):\n",
        "    filter_context_string = \"\"\n",
        "    \n",
        "    if \"sql\" in suggested_task.strip():\n",
        "        filter_context_string += str(filter_context.get(\"filter_context\", \"\")) # Context: \n",
        "        filter_context_string += str(filter_context.get(\"sql_context\", \"\").get(\"element_context\", \"\"))\n",
        "        filter_context_string += str(filter_context.get(\"sql_context\", \"\").get(\"sql_context\", \"\"))\n",
        "        \n",
        "        # filter_context_string += \"Inferred Connection: \", filter_context.get(\"connection_name\", \"\")\n",
        "        # filter_context_string += \"Inferred Schema: \", filter_context.get(\"schema_name\", \"\")\n",
        "        # filter_context_string += \"Inferred Table: \", filter_context.get(\"table_name\", \"\")\n",
        "    \n",
        "    if filter_context.get(\"validated_connection\", \"\") != \"\":\n",
        "        filter_context_string += \"\\nconnection_name: \" + str(filter_context.get(\"validated_connection\", \"\"))\n",
        "    if filter_context.get(\"validated_schema\", \"\") != \"\":\n",
        "        filter_context_string += \"\\nschema_name: \" + str(filter_context.get(\"validated_schema\", \"\"))\n",
        "    if filter_context.get(\"validated_table\", \"\") != \"\":\n",
        "        filter_context_string += \"\\nsearch string: \" + str(filter_context.get(\"validated_table\", \"\"))\n",
        "\n",
        "    else:\n",
        "        filter_context_string += filter_context.get(\"filter_context\", \"\")\n",
        "    return filter_context_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Groupchat \n",
        "allowed_transitions = {\n",
        "    user_proxy: [sql_assistant, job_assistant, reviewer_assistant],\n",
        "    sql_assistant: [executor, user_proxy],\n",
        "    job_assistant: [executor, user_proxy],\n",
        "    executor: [reviewer_assistant, user_proxy],\n",
        "    reviewer_assistant: [user_proxy],\n",
        "}\n",
        "\n",
        "groupchat = autogen.GroupChat(\n",
        "    agents=[user_proxy, sql_assistant, job_assistant, executor, reviewer_assistant], \n",
        "    messages=[], max_round=5,\n",
        "    speaker_transitions_type=\"allowed\", \n",
        "    allowed_or_disallowed_speaker_transitions=allowed_transitions,\n",
        "    send_introductions=True\n",
        ")\n",
        "\n",
        "previous_history = []\n",
        "previous_sql_output = \"\"\n",
        "\n",
        "#previous_history[-3:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### run "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#prompt = \"list distinct connections\" \n",
        "#prompt = \"whats the bigquery connection name?\"\n",
        "#prompt = \"is there a samples schema in the bigquery connection?\"\n",
        "#prompt = \"list the distinct schemas in the bigquery connection\"\n",
        "#prompt = \"count the distinct tables in the bigquery connection\"\n",
        "\n",
        "prompt = \"list the tables listed w/ claims in the name, use bigquery connection, samples schema\"\n",
        "#prompt = \"run a dq job for the claims dents and claims detail table\" \n",
        "prompt = \"run a dq job for the all the claims tables listed in that connection and schema\" \n",
        "\n",
        "#prompt = \"what tables did you last mention? \"\n",
        "prompt = \"check job status\"\n",
        "\n",
        "# fetch next task\n",
        "next_task = get_next_task(docs_df, prompt, top_n=4, to_print=False)\n",
        "print(\"NEXT TASK: \", next_task[['action', 'lookup_values', 'similarities']].to_markdown(index=False))\n",
        "suggested_task = next_task.action.iloc[0]\n",
        "\n",
        "try:\n",
        "    previous_message = str(filter_context.get(\"sql_context\", \"\").get(\"sql_context\", \"\"))\n",
        "    if previous_message != \"\":\n",
        "        previous_history.append(previous_message)\n",
        "        previous_sql_output = previous_message\n",
        "    print(\"PREVIOUS SQL OUTPUT: \", previous_sql_output)\n",
        "\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# fetch context\n",
        "filter_context = filter_step(prompt, suggested_task)\n",
        "filter_context_string = format_filter_context(suggested_task, filter_context)\n",
        "\n",
        "print(\"-\"*100)\n",
        "p = \"\"\n",
        "p += \"\\nPrompt: \" + prompt + \"\\n\"\n",
        "p += \"Please assist with the prompt above. \\n\"\n",
        "\n",
        "if previous_sql_output.strip() != \"\" and suggested_task.strip() != \"general\":\n",
        "    p += previous_sql_output\n",
        "\n",
        "if filter_context_string.strip() != \"\":\n",
        "    p += \"\\nBest Guess Inferred Context: \" + filter_context_string[0:1000]\n",
        "    print(p)\n",
        "\n",
        "cr, manager = chat(manager, p, groupchat)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ```extra```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### initial embeddings generated by openai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "from openai import OpenAI\n",
        "import pandas as pd\n",
        "\n",
        "# file \"/tmp/snippets/actions.csv\"\n",
        "# input text column lookup_values\n",
        "# output column embedding\n",
        "\n",
        "client = OpenAI(max_retries=5)\n",
        "EMBEDDING_MODEL=\"text-embedding-ada-002\"\n",
        "\n",
        "def cosine_similarity(a, b):\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "def get_embedding(text, model=EMBEDDING_MODEL):\n",
        "   text = text.replace(\"\\n\", \" \")\n",
        "   return client.embeddings.create(input = [text], model=model).data[0].embedding\n",
        "\n",
        "def search_docs(df, user_query, top_n=4, to_print=True):\n",
        "    embedding = get_embedding(\n",
        "        user_query,\n",
        "        model=EMBEDDING_MODEL \n",
        ")\n",
        "    df[\"similarities\"] = df.embedding.apply(lambda x: cosine_similarity(x, embedding))\n",
        "\n",
        "    res = (\n",
        "        df.sort_values(\"similarities\", ascending=False)\n",
        "        .head(top_n)\n",
        "    )\n",
        "    if to_print:\n",
        "        display(res)\n",
        "    return res\n",
        "\n",
        "# df = pd.read_csv(\"/tmp/snippets/actions.csv\", index_col=0)\n",
        "# df = df.dropna()\n",
        "# df['embedding'] = df.lookup_values.apply(lambda x: get_embedding(x, model=EMBEDDING_MODEL))\n",
        "\n",
        "# save embeddings\n",
        "# df.to_csv(\"/tmp/snippets/actions_with_embeddings.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "autogen",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
